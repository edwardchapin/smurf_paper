Draft of my answers preceded by "-":



Scientific Editor's Comments:

Some of he figures (e.g., Fig. 4) are too small for comfortable reading, while others (e.g., Fig. 6) are unnecessarily large. In Fig. 9, the axes need to be explained and labelled (at least the time range); I'm not sure why you include broken bolometers as "typical" - the space could be better used! In figures with many panels, it helps to label the panels (a), (b), etc. and refer to the panels by label - see the instructions to authors.

- Fig.4 had been modified, using a smaller number of sub-windows and a
  larger font it is now more readable.

- Fig.6 has been made smaller.

- Fig.9 now has axis labels and an improved caption - however, we
  elected to leave in the "broken" bolometers as this is a fairly
  typical portion of the focal plane (remembering that there is a ~69%
  yield); this gives the reader a good visual sense.

Although the paper is clearly written, it is rather wordy, and I encourage you to review the text and see whether you can express it more concisely.

There are some awkward phrases, e.g.,
"about 5000 bolometers" (don't you know how many?),
"generic science products" (surely you don't mean "generic"?),
"relatively simpler" (isn't "simpler" sufficient?),
"relatively clean" (do you mean "cleaner"?),
"nearly science-grade" (this requires explanation, or just omit this sentence, which occurs both in Section 1 and the conclusions: this duplication is unnecessary).
"The reason for this is probably due to" (you mean "This is probably due to").

- done


Assistant Editor's Comments:

Holland et al and Dempsey et al are not yet 'in press' as they have not been accepted. Please cite these as 'submitted to MNRAS'.

When submitting a revised version of this paper, it would be helpful if you could include the latest versions of the two companion papers as supplementary material for the referee.


- done as requested



Reviewer's Comments:


- We would like to thank the thorough reviewer for identfying several
  areas that needed clarification, and perhaps more importantly, three
  careless errors (which led to incorrect units in Figs.1 & 2, and
  confusing results in Figs.11 & 20), and an ommission (impact /
  treatment of the response of the bolometers and data acquisition
  system). Detail responses follow.

Overall: The paper will be a useful reference for SMURF users and clearly represents a lot of work. Forgive me for only picking on the residual bits that I see need improving, but this will be the most efficient way to get to the end of this process.

Although I appreciate the division between this paper (data processing) and others to do with the instrument etc, it would be useful from a user perspective to have some sense of number of working channels, some parametrization of survey depth as a function of time&area -- using your "white noise limit", number of bad detectors, statistics on spikes etc. A lot of material is presented in section 2 to characterize the bolometer data. It would be very useful to bring it home with an focal-plane-wide characterization summary.

- A new, short, Section 3.4 has been added to address this.

In the examples it seemed like the "default" pipeline was never the correct one to use. Is there a case where the default is the one to use?

- Perhaps "default" is the wrong word - in reality, the description in
  Section 3 is of the "baseline" parameters for the reduction, which
  are then added-to for specific reduction, as in the examples from
  section 4. The word "default" has been replaced with "baseline"
  throughout, and clarification added to the intro of Section 4 on
  example reductions.

I really think it's useful and proper to cross-reference the millimeter kilopixel arrays with comparable data volumes and science overlap (SPT and ACT) and associated data processing literature out there. See specific comments below.

- Agreed. The artificial division between "submm" and "mm"
  is not useful in this paper, and SPT and ACT are certainly relevant
  comparisons.

+ Abstract: "… 200 Hz. This rate is much greater…"

This may be a true statement, but it neglects that very analogous arrays have been deployed in the millimeter. I'm thinking of SPT and ACT, which are arguably more similar to SCUBA2, by virtue of their kilopixel scale and multiplexed readout, than the previous generation submillimeter. SPT downsamples to 100 samp/sec. ACT writes to disk at 400 samp/sec and then downsamples to 200 samp/sec for analysis. SCUBA 2 has more pixels than either of these experiments (factor of 10 for SPT and 3 for ACT). Then again, as written the sentence is true, though maybe "data rate" would be better than "rate" which would make the claim stronger, even when including the millimeter kilopixel instruments.

- Agreed, and re-worded so that SCUBA-2 is now "the largest of a new
  generation of multiplexed kilopixel bolo cmaers"

+ Sec 1 Para 4: "``cross-linking'',in which each portion of the map is sampled on a range of temporal time scales"

This is an odd definition of cross-linking. The essence of cross-linking, as it's name implies, is that the sky in a given pixel is sampled in various directions, thus connecting pixels in the map isotropically and producing nice symmetric noise properties (and in the case of a mapper like SMURF also symmetrizing the transfer function). Coming back to timescales, in the SMURF approach any timescale corresponding to a revisit of a pixel (except at map boundaries) is going to be wiped out due to filtering/ common mode removal. In this sense the main (nice) benefit of the X-linking is isotropy. I realize this is a general discussion of mapping, but even then, it's really that you're connecting all the pixels and thus isotropizing the covariance in map space. In principal the different time scales could come into play in a full ML mapper, but the different time scale visitations could be achieved without cross-linking. For these reasons, I would emphasize position angle variation etc, as you do in most parts of the following text -- don't lead in with "different time-scales".

- Agreed, and description modified.

+ Sec 1 Para 5: "baseline removal and other simple filters"

This would be a nice place to cite SPT (Say, http://adsabs.harvard.edu/abs/2011ApJ...743...90S).

- Done.

+ Sec 1 Para 6: "maximum likelihood techniques"

This would be a nice place to cite ACT (Say, http://adsabs.harvard.edu/abs/2011ApJ...729...62D or http://adsabs.harvard.edu/abs/2012arXiv1208.0050D -- though this is not accepted).

- Done (although used earlier Fowler 2010 as a reference).

+ Sec 1 Para 7: "merely linear in the number of iterations needed, while using a comparable amount of memory"

I got tripped up here. Maybe you can clarify. Here are my thoughts:

The first part seems like a tautology: of course iterative methods scale as the number of iterations. Often times, the bottom line is that inversion is essentially impossible computationally (for a map with a few million pixels say) and the iterative methods let you go forward. The less trivial questions are: how long do the iterations take and how many iterations do you need? The iteration time is going to scale as the time ordered data length (or say NlogN for FFT based methods). You'll need more iterations to get to larger scales (e.g., if you're trying to go beyond your array footprint): for most techniques that I've encountered, small scales will be recovered in ~10 iterations while larger scales can take thousands of iterations, depending on the instrument etc.

Is the second bit true? Does the matrix inversion really take the same amount of memory as the iterative techniques for all map/time ordered data sizes?

- Agreed. Added some discussion of how algorithms scale.

+ Sec 1 Para 8: "single high-end desktop computers"

It would be useful for SMURF/SCUBA2 users to know an example of what would qualify as "high-end". (Number of processors and amount of memory would probably be enough to give the idea.)

- added a setence describing the machine used for analysis in the
  paper: 64-bit, 8 cores, 2.7GHz, 48 Gb of RAM.

+ Sec 2.2, 2.4, Figure 1, etc: e.g., "It is also likely that a varying atmospheric contribution is also included in these eigenvectors…"

I didn't get a good feeling from these sections as to how large a signal (say in pW over some time period) the atmospheric drift is in the data. There was a lot on scan synchronous noise (either before debugging the cryogenic problem or after where magnetic pickup seems to dominate). I was surprised to see that the change in elevation didn't give a signal due to changes in airmass. Could you give a better sense of how the atmosphere contributes? A possibly useful figure (though I know there are a lot of figures) is to plot time streams or spectra taken during different conditions (parametrized by PWV?). Even to just to give some quantitative sense of the magnitude in of the atmosphere in the data would be useful. The first components of the PCA were said to be scan synchronous noise dominated "presumably magnetic field pickup"… Is the atmosphere scan synchronous (e.g. b/c of airmass considerations?)… I was just left wondering here. From figure 1, right panel, it looks like after the magnetic field is removed, then the white noise totally dominates atmospheric drift. Figure 6 looks like a good example atmospheric drift maybe???

- After reading your comments we realised that the paper is lacking
  some detail on the atmosphere. While we felt that a really thorough
  analysis was beyond the scope of this paper (which is already very
  long!), we added a couple of things to help. First, Fig.1 has
  additional panels showing the line-of-sight opacity from the water
  vapour monitor. This *does* show the effect of elevation, so we've
  backed-off slightly on the claim that everything is magnetic field
  pickup (although it is all still scan-synchronous). Next, we've
  added a new Fig.4 and Section 2.4, in which we show some bolometer
  data over a much longer period (20 min.), in which the
  long-timescale behaviour is clearly dominated by underlying
  atmospheric variations. In the end, however, it is the
  higher-frequency scan-synchronous stuff that matters most for
  map-making. Finally, we re-ran the PCA code after first doing extra
  cleaning on the data (the first version of the plot had glitches in
  it which weren't particularly insightful). We were able to reduce
  the panels down to 9 from 12 to make them more readable, and the
  units are also now correct (see below). The discussion has been
  modified in Section 2.5 to reflect these changes, and the modified
  interpretation of sky noise.

+ Figure 2: "2x10^5 pW Hz^{-1}"

This number (and accompanying y-axis) seems reasonable given the plots in figure 1 (white noise is 2x10^4 pW Pk-Pk), but the white noise in Figure 6 looks much smaller (white noise is 0.01 pW Pk-Pk). Also glancing at Fig 9 in the companion paper (Holland et al) suggests NEPs of ~ 10^-16 W-rtHz or 10^-4 pW-rtHz. The latter numbers are more what I'm used to seeing in bolometer systems at these temps. Please clarify units. Is there a calibration missing…?

- Thank you for catching this! The original plots were not
  flatfielded, so they were in rather useless DAC units. Figs.1, 2,
  and the PCA plot have been re-made with the correct, sensible units
  of pW.

+ Sec 2.4, Para 4: "resembles the scan pattern in the right-hand panel of fig. 4"

I don't see a scan pattern in figure 4…

- Fixed.

+ Sec 3, Para 3: "the map is used to estimate and remove the astronomical signal contamination in the bolometer data…"

This seems like a oxymoron: astronomical signal contamination. I think it'll confuse people. Maybe "remove the astronomical signal from the bolometer data, leaving only white noise (NOI)". Or something like that. (It's also weird to talk about data which is only noise as "clean"!)

- Re-worded.

+ Figure 5. "DC steps"

There are enough acronyms running around this paper that it took me a while to realize what this meant. And that's only because I've worked in a lab where DC is a normal (and completely overloaded) term.  Maybe just drop "DC". You talk about "step correction" later. So probably "step" is less confusing that "DC step", which I don't see come up later.

- Agreed, term changed to "steps" throughout.

+ Sec 3.1

I didn't see any discussion of accounting for the time constants of the detectors or of the MCE anti-aliasing filter. Do you not deconvolve these as an initial step? Seems like this would give you a non-trivial transfer function at small scales if you did not…. (even if you have your nice cross-linked isotropic transfer function, you would have the unfortunate characteristic of wiping out small scale structure). Please clarify.

- This was a fairly major ommission in the first version. This is an
  effect that we considered long ago, and had been ignoring due to
  early estimates that showed they should be insignificant. In fact,
  there was no special code in SMURF to handle the de-convolution of
  these response functions. However, we have calculated and simulated
  these effects, finding that it can actually be quite significant at
  450um (15% attenuation of a point source at the maximum scan speed
  of 600 "/s). Fortunately this will not affect many users, but it is
  important to provide as a warning. The current development branch of
  SMURF now has code to handle this correction, but it is not in the
  release that is otherwise described in this paper. The details of
  the response are now added to Secion 2.1, and we've added Section
  3.1.7 to discuss the impact on maps.

+ Sec 3.1.1 Para 2: "fractional samples"

What is a fractional sample? Would be nice to clarify in text.

- The way input samples are divided in the lower sample rate output
  stream is now clarified in the text.

+ Sec 3.1.2, 3.1.3: de-spiking and step correction

For the SMURF user, it would be nice to know how often this occurs in a dataset. (Related to my general comment of giving an idea of the array/observation properties of the time stream data) It would be nice to have some statistics.

- some example values are now given in the new section 3.4.

+ Sec 3.1.4, Para 1:  "bad values are then replaced by linear interpolation between…"

Are these given zero weight in the map?

- correct. modified in the text.

+ Sec 3.1.4, Para 3: "The default number of samples modified at each end of the time-series is given by half the ratio of the sampling frequency to the lowest retained frequency."

Unclear at this point what "lowest retained frequency" means -- please clarify.

- This frequency corresponds to the edge of the high-pass filter that
  we used. Now clarified.

+ Sec 3.2, Para 4: "During pre-processing, the map-maker divides by f_i once. Then in each iteration, the map-maker divides by f_i…"

Seems like the data is repeatedly being divided by the flatfield and digitization constant… especially confusing b/c in the second paragraph we have "applied once during preprocessing"

- This was a mistake in the text. This division only happens once at
  the start. Now corrected.

+ Sec 3.2.1, Para 5: "cases where the data is dominated by magnetic field pickup"

Why not remove magnetic field pickup as a preprocessing step? It's generally on scales larger than you'll ever recover with SMURF and you can use the tricks (dark squids etc) that you have in the DKS module…

- It wouldn't work as a pre-processing step because the fit would be
  quite noisy due to the presence of other signals (e.g. bright
  astronomical sources).

+ Sec 3.2.1, Para 6: "Bolometer values that were flagged… are, however included in the new common-mode estimate"

Again, couldn't this exclusion be a pre-processing step (after the hypothetical DKS preprocessing) so you don't have to deal with these bad detectors?

- Sometimes yes, sometimes no. See comments below.

+ Sec 3.2.6, Para 2: "not all columns have working dark squids"

Glancing at Holland et al, it looks like the arrays have 60-80% yield, could one of the dead detectors be used as a dark channel (or are they dead because the mux is dead?).  In fact it's nice to have a "dark squid" on all rows and columns to get rid of noise (low and high frequency) related to the electronics.

- Yes, this was actually an idea at one point. However, since we had
  little success with the working dark squids, it was decided that it
  wasn't worth the effort.

+ Sec 3.2.6, Para 2: "necessitating the continued use of FLT… they [DKS/TMP] are not generally used"

Again, though, why not do this as a preprocessing step together with identifying all the bad non COM detectors? The dark detectors should not only carry information about the magnetic field pickup, but also contain high-frequency information related to row addressing and column readout that can be used to decontaminate the data. (Even if FLT does tend to kill everything on these scales, this could be a COM improving step where you can toss the bad detectors that don't follow common mode)

- In principle we agree with most of your comments on the dark squids;
  they should carry useful information that can beat-down the noise at
  all frequencies (not just the lowest frequencies). In practice the
  results were all over the place. Sometimes it helped a bit, but
  often the noise increased. It's rather frustrating that this did not
  work any better, but ultimately we had to resort to high-pass
  filtering to get good, repeatable results. We have tried to express
  this more clearly in this section.

+ Sec 3.3, Para 1: "Once the white noise, n_i^w(t) has been measured from the bolometer PSDs in NOI"

Is the noise time stream "model" n_i^w(t) really estimated from the PSDs or is this the weight w_i (or variance sigma_i^2…) that is estimated from the PSDs?

- We had some poor word choices in this section. We now clarify that
  "NOI" is just measuring the PSD to estimate a scalar variance, which
  later gets turned into the weight for map estimation, w_i = 1/sigma_i^2. 

+ Sec 3.3, Para 1: "requsted threshikd"

typo

- fixed

+ Sec 3.3, Para 2: "does not necessarily correlate with convergence of the most important model component"

It was not clear to me when one would every use the "chi-sq" convergence criterion. The map-based one looks much more conventional to me (a la the conjugate gradient techniques). Please include a blurb on when the SMURF user would use the "chi-sq" convergence criterion in addition or as replacement for the map-based one.

- Correct. The chi^2 test is really more useful as an introduction to
  model degeneracies - it isn't useful as a stopping criterion as you
  say. This section has been re-written.

+ Sec 4.1, Para 4: "This example illustrate the need…"

typo

- fixed

+ Sec 4.1, Para 4: "a good, simple prior is to constrain the map to a value of zero away from the known locations of emission"

How does not masking a bright source prevent filter ringing? are you masking all the "sidelobes" of the filter? Please describe the mechanics of this "simple prior" in an additional one or more sentences.

- We have added some clarification in a new paragraph. In essence, the
  bolometer data are capable of constraining *differences* in the map
  up to the scale of the filter. Constraining (empty) portions of the
  map to zero provides references for those differences so that you
  get a reasonable answer out.

+ Figure 10:  "Same as (a) but using 50 iterations…"

I think you mean 100 iterations?

- Correct, fixed.

+ Figure 10: "Uranus peak 0.15 pW" -> "Uranus peak 0.27 pW"… "little attenuation of the source flux"

Can you explain why adding an extra filter to the process (just common mode -> filter + common mode) results in a increase (.15->.27 pW) in the peak brightness? Is there small scale noise (a stripe?) that is randomly knocking down the signal in the common mode removed case that gets nixed by the filter? Also can you quantify "little attenuation"? What do you expect for the uranus brightness in this measurement?

- Another careless mistake. The top two plots were made without
  extinction correction, while the bottom-two were. This figure has
  been re-made (note that the peak fluxes have changed since we were
  also forgetting to include some recent changes to the calibration),
  and things are more sensible: the extra filter gives you a *lower*
  flux now, as you correctly expected.

+ Sec 4.2, Para 1: "whitening (essentially flattening) the map"

flattening in Fourier space? As written a neophyte might think you mean making the map flat=constant! this is a potentially confusing parenthetical piece of jargon… maybe just drop since you describe it later?

- re-worded as a "Fourier-space filter to suppress residual large-scale noise".

+ Sec 4.2, Para 2: "However, the way this filter affects point-sources is both well-known, and linear."

Presumably this depends on the cut-off of the pre-processing filter. Does SMURF tell the user what this is going to be or does SMURF provide the tools to make the synthetic source so they can test it out? It would be useful to include such information.

- SMURF has a facility for adding in synthetic sources to real data
  on-the-fly. This is the mechanism we have used (and recommend to
  users), and we now state this in the text.

+ Figure 12: "this search radius"

8"? From the wording, it's unclear what is the "search radius". Please clarify.

- Correct, fixed.

+ Sec 4.2, Para 8: "Under this assumption, cross-correlation between between the map and the known PSF…"

This statement also assumes white noise.

- caveat added.

+ Sec 4.2, Para 8: "For the case at hand, the effective PSF for the whitened maps is given by the dotted curve in Fig. 13."

This is confusing language. Fig 13 list the dashed line as "whitened". It is true though that the dotted curve gives the correct PSF for your matched filter, since it's the one that corresponds to the pre-filtered beam. The matched filter in Fourier space is B(k)N^-1(k), where the N^-1(k) is your pre-whitening step (dividing by the power spectrum of the noise). The B(k) is the PSF before the whitening. It seems like you say this here, but it's a bit confusing with the figure labeling. Maybe it would be useful to include a simple equation like  \Phi(k) =  B(k)N^-1(k) or similar to describe your total operation?

- Yes, we are essentially breaking down the typical B(k)/N^-1(k)
  matched-filter expression into two steps. We've clarified the text
  here.

+ Sec 4.3, Para 2: "Much like the reduction of a point source without any prior constraints (Fig 10c), degeneracies on scales larger than the array footprint and filter produce ripples in the map."

This sentence confused me. In the source case you had two things: 1) degeneracies between your map and the common mode that introduced very large modes, which you eliminated by wiping out those scales with a filter before estimating the map. And then you found 2) that the filter was ringing and producing ripples in the map. I think you're saying you have these two problems here but the sentence, as written, made it seem like the two were somehow interacting or somehow the same thing. Maybe something more explicit like "Much like…, mapping extended emission would suffer from degeneracy between the common mode and map (if no filter were used) and, once the filter is introduced, ringing around sources.

- Yes, this is confusing because we're mixing two ideas. In fact, both
  the high-pass filtering and the common-mode removal can have similar
  problems with degeneracies, but it depends on the scale of the
  filter relative to the array footprint. Some new text in both
  Section 4.1 and 4.3 now explains this better.

+ Sec 4.3, Para 4: "By the final iteration, most of the obvious structures in the data are encompassed by the mask…"

This is opposite the bright source case (example 1) in which everything but the source was masked. Since you're drawing an analogy (see previous comment) with the point source reduction, would you add a bit more description to explain why you use the "opposite" mask here?

- Our description was very confusing here! Due to a poor word choice
  it sounds as though we were using the inverse of what we really
  do. The portion of the map contrained to zero is where the SNR is
  *below* some threshold (so similar to the case of Uranus). This has
  been clarified.

+ Fig 18

Why is the jackknife noise higher than the map noise at small (<25") angular scales?

- Another careless mistake. The average maps used variance weighting,
  whereas the jackknife (difference) maps did not, leading to greater
  noise. This has now been fixed and the plots look more sensible at
  small scales.

+ Sec 4.3, Para 5: "at least a cosmetic sense"

in a cosmetic sense?

- Fixed.

+ Sec 4.3, Para 9: "in which the SNR threshold of 5 is again used to constrain the map"

A bit confusing, esp. b/c I only found S/N=5  mentioned before in a caption. Maybe "S/N threshold is used to define the extended brightness mask and constrain the map" or something a little more explicit like this...?

- Extra text added to clarify that pixels below this threshold are set to zero.

+ Sec 4.3, Para 13 "In this case the noise… in the left panel of the third row"

Would really help to give figure numbers here.

- Added.

+ Sec 4.3, Final Paragraph: "such structures have a chance to grow"

too vague -- "such structures" could be interpreted as the large scale astro structure. Maybe something like "degeneracies with the common mode have the chance to show up in the map"

- Text changed to something resembling this suggestion
