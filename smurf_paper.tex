%------------------------------------------------------------------------------
% SMURF Paper
%------------------------------------------------------------------------------

\documentclass[useAMS,usenatbib,nofootinbib]{mn2e}

% this is needed to get pdftex to work properly!
\usepackage[pdftex]{graphicx}

\usepackage{amsmath}
\usepackage{url}
\usepackage{natbib}
\usepackage{rotating}

% --- Some user defined macros ------------------------------------------------

% journals
\newcommand{\apj}{\rm ApJ}
\newcommand{\apjl}{\rm ApJL}
\newcommand{\apjs}{\rm ApJS}
\newcommand{\aaps}{\rm A$\&$AS}
\newcommand{\aap}{\rm A$\&$A}
\newcommand{\aapr}{\rm A$\&$AR}
\newcommand{\mnras}{\rm MNRAS}
\newcommand{\aj}{\rm Astron. J.}
\newcommand{\araa}{\rm ARAA}
\newcommand{\nat}{\rm Nature}
\newcommand{\pasj}{\rm PASJ}
\newcommand{\pasp}{\rm Publ. Astron. Soc. Pac.}
\newcommand{\prd}{\rm PhRvD}
\newcommand{\ASP}{\rm ASP Conference Series}
\newcommand{\CASP}{\rm Comm. Astrophys. Space Phys.}
\newcommand{\astroph}{\rm astro-ph/}


% common acronyms etc.
\newcommand{\snr}{SNR}
\newcommand{\scuba}{SCUBA-2}
\newcommand{\rms}{RMS}

% these are approximately less-than and greater-than symbols
\def\lsim{\mathrel{\lower2.5pt\vbox{\lineskip=0pt\baselineskip=0pt
          \hbox{$<$}\hbox{$\sim$}}}}

\def\gsim{\mathrel{\lower2.5pt\vbox{\lineskip=0pt\baselineskip=0pt
          \hbox{$>$}\hbox{$\sim$}}}}


% ----------------------------------------------------------------------------

\title[SMURF: an iterative map-maker for SCUBA-2]{The Sub-Millimetre User
Reduction Facility: an iterative map-maker for SCUBA-2}

\author[Edward~L.~Chapin~et~al.]{
  \parbox[t]{\textwidth}{
    Edward~L.~Chapin$^{1,2}$\thanks{E-mail:~echapin@phas.ubc.ca},
    David~S.~Berry$^{2}$,
    Andrew~G.~Gibb$^{1}$,
    Tim~Jenness$^{2}$,
    Douglas~Scott$^{1}$,
    Frossie~Economou$^2$
  }
  \\
  \\
  $^{1}$Dept. of Physics \& Astronomy, University of British Columbia,
  6224 Agricultural Road, Vancouver, B.C. V6T 1Z1, Canada\\
  $^{2}$JointAstronomy Centre, 660 N. A`oh\={o}k\={u} Place, University
  Park, Hilo, Hawaii 96720, USA}

\begin{document}

\label{firstpage}

\maketitle

\begin{abstract}
  We describe the properties of data from the Submillimetre Common
  User Bolometer Array 2 (SCUBA-2) taken during the Shared Risk
  Observing period from 2010 February 11 to 2010 March 25, and the
  production of maps using the Sub-Millimetre User Reduction Facility
  (SMURF). Our iterative approach is similar to those used to reduce
  data from other ground-based submillimetre telescopes in recent
  years, and recovers angular scales up to the size of the array
  footprint (approximately 2\,arcmin). The method is also able to
  achieve the theoretical white-noise limit of the instrument for
  point-source studies.
\end{abstract}


\begin{keywords}
chicken, chicken: chicken, chicken: chicken -- chicken
\end{keywords}

%------------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
%------------------------------------------------------------------------------

\textbf{Guideline for figures:}

\begin{itemize}
\item minimize excess whitespace around the figure to optimize usage of space
\item ensure a decent line thickness
\item use a Times font for labels with a thickness that resembles the
  text in the body. The font size should be similar to the main body
  text or larger
\item avoid using colour unless it is necessary for emphasis
\end{itemize}

\textbf{start intro here...}

The Submillimetre Common User Bolometer Array 2
\citep[\scuba,][]{holland2006} is a new instrument for the 15-m James
Clerk Maxwell Telescope (JCMT) on Mauna Kea, Hawai'i. The camera
simultaneously images the sky in two broad bands centered over 450 and
850\,\micron. The focal planes at each wavelength are populated with 4
rectangular subarrays, consisting of $40 \times 32$ bolometers each,
and subtend a nearly contiguous $4' \times 4'$ footprint on the sky.
Ultimately, the instrument will also be equipped with a Fourier
transform spectrometer \citep[FTS-2,][]{gom2010}, and a polarimeter
\cite[POL-2,][]{bastien2005}. This paper describes the Submillimetre
User Reduction Facility, SMURF, a software package for reducing the
imaging data. The reduction of FTS-2 and POL-2 data will be described
at a later date once these additional instruments have been
commissioned. The details of the instrument design, performance, and
its calibration are given in two companion papers: \citet{holland2011}
and \citet{dempsey2011}.

Over the last twenty years, observations in the submillimetre band
(defined here to be 200--1000\,\micron) have helped revolutionized
several important areas of astrophysics including: the
characterization of early stages of star-formation by identifying the
dense, cold regions in molecular clouds where stars may eventually
form; discovering through blind surveys a class of massive dusty
star-forming galaxies in the early ($z>2$) Universe, now referred to
as submillimetre galaxies, or SMGs; and identifying debris disks
around nearby stars, the early stages of planet formation.  With
$\sim10000$ total detectors, SCUBA-2 is presently the largest
submillimetre camera in the world, and the fastest wide-area
ground-based submillimetre imager.

Most submillimetre imaging cameras to date have used bolometers to
maximize sensitivities: each pixel consists of an efficient thermal
absorber to collect photons that have passed through a broad-band
filter, and the signal is read-out using a thermometer. The
high-frequency broad-band noise in modern systems, such as SCUBA-2, is
limited by photon noise from the instrument and ambient
backgrounds. The low-frequency noise, however, is dominated by sources
such as the low-frequency drift in the readout electronics, and drifts
in the mean background levels (such as the atmosphere for ground-based
instruments). The process of making a map therefore amounts to
separating the component of the bolometer signals that is produced by
astronomical sources, from the non-white (and therefore correlated)
contaminating noise components. Fortunately each has a particular
profile: astronomical sources are typically fixed in a sky reference
frame (to a good approximation), whereas other noise sources are often
correlated in time, and usually from bolometer-to-bolometer. ...also
mention scan strategies...

There are numerous ways to attack this problem, most of them evolving
out of techniques that were developed to produce maps of the
large-scale anisitropies in the Cosmic Microwave Background (CMB).  Styles
of map-making: maximum likelihood \citep[e.g.,][]{patanchon2008};
iterative -- see \citet{johnstone2000} implementation of
\citet{wright1996} to remove chop, \citet{kovacs2008} for SHARC-2
etc.; de-correlation using PCA analysis
\citep[e.g.][]{laurent2005,scott2008,aguirre2010}.

\scuba\ is presently unique in terms of the shear volume of data that
it produces on a nightly basis, and the fact that it is observing a
wide variety of science targets for multiple users (unlike dedicated
CMB experiments, for example). For this reason, SMURF has been
designed to use a faster, and less memory-intensive algorithm than
CMB-style maximum likelihood map-makers, although it is less accurate
at reproducing large-scale structures. The basic procedure is to
iteratively model and remove most of the bolometer-bolometer
correlated signal components and independent low-frequency noise, and
then regrid the residual signals. This method is similar in spirit to
the Comprehensive Reduction Utility for SHARC-2
\citep[CRUSH,][]{kovacs2008}, and the pipeline developed for the
Bolocam Galactic Plane Survey \citep{aguirre2010}. While most of the
analysis described here applies equally to all of the data taken
to-date with \scuba, we note that the SCUBA-2 Shared-Risks Observing
period (S2SRO) which took place from 2010 February 11 to 2010 March 25
used only a single subarray at each of the two observing wavelengths,
and had considerably different noise properties and poorer performance
than the fully-commissioned instrument in 2011.

\textit{May want to note that the pipeline is discussed elsewhere,
  e.g. Gibb et al 2006.}

%------------------------------------------------------------------------------
\section{\scuba\ data properties}
\label{sec:data}
%------------------------------------------------------------------------------

In this section we summarize how \scuba\ bolometers detect temperature
variations (Section~\ref{sec:bolos}), typical \scuba\ scan strategies
(Section~\ref{sec:scan}).

%--------------------------------------------------
\subsection{How the \scuba\ bolometers work}
\label{sec:bolos}
%--------------------------------------------------

While the details of how \scuba\ works is described in
\citet{holland2011}, and its calibration in \citet{dempsey2011}, we
summarize the basic concepts relative to map-making here.

The \scuba\ bolometer temperatures are read-out using superconducting
transition-edge sensors (TESs), with critical temperatures
$T_\mathrm{C}$ (...typical value...). The range about $T_\mathrm{C}$
in which the transition from a superconductor to a normal material
occurs is extremely small (...typical value...?). If the devices are
biased to a temperature within this transition on large time scales,
then tiny temperature fluctuations on short time-scales will produce
significant changes in the impedence. The entire SCUBA-2 focal plane
is therefore held at a \emph{base temperature} somewhat below
$T_\mathrm{C}$. The TESs then have a small electrical voltage biases,
and individual pixel heaters (using a current bias to apply the
desired Joule heating) are used to place the detectors in the
transition. Only a small detector bias voltage is required since the
heaters compensate for most of the variations in the base temperature,
and other ambient loads (like the atmosphere).  Small temperature (and
therefore impedence) fluctuations produce a changing current through
the TESs. The resulting changing magnetic fields are then amplified
using chains of superconducting quantum interference devices (SQUIDs),
and the larger output current is finally digitized. Each detector has
its own SQUID for the first-stage of the amplification, but the
remaining stages occur within a common chain of SQUIDs for each column
of 40 detectors. Each of the 32 rows is then read-out in sequence at a
row-visit frequency of ...kHz?... This multiplexing scheme greatly
reduces the number of readout and bias lines to each detector, making
such large-format bolometer array designs feasible. It should also be
noted that there is an additional 33rd row of SQUID reasouds that are
not connected to TES. These ``dark SQUIDs'' are used to track
non-thermal noise sources that are common to each column's amplifier
chain.

The response of the output digitized current, $I$, with respect to a
change in the power $P$ incident on the bolometer, $dI/dP$ depends on
the efficiency of the thermal absorber, how well the TES is biased
into the transition, and the gain of the SQUID amplifiers. Since there
are variations in $T_\mathrm{C}$ across each subarray due to
manufacturing difficulties, considerable effort has gone into
dynamically applying position-dependent biases as well as an over-all
focal plane bias to make as many detectors useable as possible
\citep{holland2011}. In practice, the response of a given detector can
therefore vary somewhat from observation to observation depending on
how the bias and amplification parameters were set up. To track these
responsivity changes, at the start of each observation, the current is
ramped through the bolometer heaters such that $dI/dP$ can be
measured. Note that the bolometers may have a slightly different
response to heater power due to conductance as compared to the
response to optical power, so additioanl efficiency corrections must
also be obtained. While not discussed further in this paper, details
may be found in \citet{dempsey2011}. We will refer to measurements of
$dI/dP$ across the focal plane as the
\emph{flatfield}. Fig.~\ref{fig:sensitivities} shows the flatfield for
each focal plane.

\begin{figure}
\vspace{2cm}
\textbf{This will be a figure showing s4a and s8d responsivity maps}
\vspace{2cm}
\caption{The response of each bolometer to known input heater power
  for a typical night during S2SRO. The s4a subarray had a
  particularly large variation in $T_\mathrm{C}$ towards the
  bottom-right which resulted in lower responsivity, and
  correspondingly larger noise. The s8d array also suffered
  significant variations across the focal plane, in particular
  removing larger number of useful detectors from the bottom-left.}
\label{fig:sensitivities}
\end{figure}


In general, bolometers have two predominant noise components: (i) the
effectively white thermal noise produced by the phonon and optical
loading on the absorbers; and (ii) and longer-timescale drift
resulting from slow variations in the mean load (e.g. fridge,
telescope, or atmospheric changes), and also the readout
electronics. As such, the spectrum corresponding to a bolometer
time-series exhibits a ``$1/f$'' shape below some knee frequency,
$f_\mathrm{knee}$, and is relatively flat at higher frequencies. Other
smaller effects may include line features in the spectrum (a common
culprit is pickup from AC mains, or microphonic signals induced by
mechanical vibrations), and finally a roll-off towards the Nyquist
frequency caused by the anti-aliasing filter that precedes
digitization.

The final conversion of power received by the detector into
astronomical flux units requires two more fundamental corrections: (i)
atmospheric extinction (due to atmospheric water vapour) must be
compensated for; and (ii) the corrected power is related to an
astronomical flux density through the effective area of the detector,
the spectral response of the bolometers (bandpass), the spectrum of
the source, and additional optical efficiency factors -- a \emph{flux
  conversion factor} (FCF). Extinction is measured, in most cases,
using the JCMT line-of-sight water vapour radiometer (WVM) which is
calibrated using skydips and the CSO 225\,GHz tipper. The FCF is
determined empirically using measurements of standard sources. For
further details on these calibration steps, see \citet{dempsey2011}.

%--------------------------------------------------
\subsection{Scan strategies}
\label{sec:scan}
%--------------------------------------------------

In order to observe astronomical sources, it is clearly advantageous
that the signals appear in the region of the spectrum that offers the
highest \snr, namely frequencies above $f_\mathrm{knee}$. This effect
is achieved by modulating the signal; the original SCUBA primarily
used the JCMT chopping secondary to place sources into the spectrum at
about $\sim$10\,Hz. The problem with this strategy was that SCUBA was
ultimately limited to recovering information on angular scales smaller
than the chop throw (a maximum of a couple of arcmin)... cite
somebody. The primary approach to map-making with SCUBA-2 is instead
to slew the entire telescope at such a rate that angular scales of
interest are crossed on time scales shorter than $1/f_\mathrm{knee}$,
similar to other ground-based instruments (MAMBO, BOLOCAM, SHARC-II,
AzTEC, LABOCA,...). The desire to scan rapidly has two basic
constraints: (i) the telescope cannot physically move faster that
several hundred arcsec\,sec$^{-1}$ without requiring unacceptably
large accelerations to change direction, and (ii) bolometers are
designed with a thermal time constant that balance response with
sensitivity. Taking these constraints into consideration, SCUBA-2 was
ultimately designed to work at a maximum scan velocity of
600\,arcsec\,sec$^{-1}$. The data acquisition system has a target
sample rate of 200\,Hz, which at that scan velocity results in a
sample every 3.0\,arcsec, or approximately one third of the
450\,\micron\ diffraction-limited full-width at half-maximum (FWHM) --
a typical rule-of-thumb for adequately sampling a Gaussian point
spread function (PSF). Similarly, the SCUBA-2 bolometers are designed
to have a time constant of 2 ms \textbf{check that number!} in order
to avoid distorting PSF-sized sources at this maximum scan rate.

The scan strategy is designed, primarily, to mitigate the effects of
$1/f$ drift. The simplest scan one might use is a raster pattern, or a
series of parallel passes across this field. However, such a scan
yields extremely poor performace for structures that are transverse to
the scan direction, since a long time will have ellapsed between each
parallel step, and the signal will therefore be totally dominated by
the $1/f$ noise. In addition, other noise features may appear in the
data periodically. With such a regularly spaced periodic scan
strategy, these noise features can easily conspire with the scan
pattern to produce similarly periodic noise structures in the map, or
\emph{scan-synchronous} noise. It has therefore been recognized that
scan strategies that visit each region of the map on as many different
time scales, and scan angles as possible offer the best protection
against large-scale noise (...cite some people...); such strategies
are said to offer good \emph{cross-linking}. As a tradeoff between
truly random scan patterns and feasibility of implementation taking
into consideration both mechanical and scheduling constraints, SCUBA-2
presently has three primary scan modes: PONG (cite Borys et al.),
Lissajous, and Daisy... (more citations? Kovacs et al. has a nice
description... also CMB literature).

%-------------------------------------------------
\subsection{Typical bolometer signals}
%-------------------------------------------------

In Fig.~\ref{fig:bolos_mix} we show sample time series from single
bolometers in each of the subarrays, as well as variations in the
mixing chamber temperature (as a proxy for the focal-plane
temperature), and the telescope pointing during a PONG scan.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{bolos_point_mix.pdf}
\caption{A comparison between single bolometer time series in each
  band (top two plots), with the mixing chamber temperature and
  azimuth/elevation pointing offsets. There is a strong correlation
  between the bolometers and the roughly $\sim$25\,s oscillation in
  the fridge, but not with the telescope motion. Also note that the
  total power in this primary signal is similar at both 450 and
  850\,\micron, pointing to an internal rather than an external
  source. The grey signals overplotted in the top-two panels show the
  residual time-series after removing the common-mode signal. This
  operation shows: (i) that most of the low-frequency signal is common
  to all of the bolometers; and (ii) the non-correlated, and
  predominantly white noise at 450\,\micron\ is significantly larger
  than at 850\,\micron.}
\label{fig:bolos_mix}
\end{figure}

Both bolometers share significant long-timescale structure
($\gsim10$\,s) that appears to be related to variations in the fridge
base temperature, although the similarity is clearly greater at
850\,\micron. In this particular case, the total power in the
fluctuations at 450\,\micron, from $-0.04$\,pW to $+0.08$, are larger
than the $-0.03$\,pW to $+0.05$\,pW fluctuations at 850\,\micron. Such
behaviour might be expected if there is a comparable varying thermal
load from the fridge at each wavelength, but a larger contribution of
atmospheric variations through the 450\,\micron\ bandpass filters. We
also note that there is no obvious strong correlation between the
low-frequency signal structure, at either wavelength, with the
telescope motion.

The low-frequency signal component of the bolometer signals is also
highly correlated amongst bolometers in the same subarray. We have
calculated a common-mode signal, $\mathbf{c}(t)$, as the average time
series of all the working bolometers. We then fit the amplitude of
$\mathbf{c}(t)$ at each wavelength to the signals shown in
Fig.~\ref{fig:bolos_mix} and remove it, yielding the grey residual
signals. These residuals are quite flat, although still with
noticeable variations. The white noise is also apparent, and larger at
450\,\micron\ as one would expect from the larger backgrounds compared
to 850\,\micron.

Next, we produce power spectral density (PSD) plots for several
representative bolometers in Fig.~\ref{fig:pspec}. To produce this
figure, we follow the convention that the PSD as a function of
frequency, $\mathbf{P}(f)$, is normalized such that the integral over
frequency gives the same variance as the time-series variance across
the full time series. In other words, given a bolometer signal
$\mathbf{b}(t)$,
%
\begin{equation}
\label{eq:psd}
\langle\mathbf{b}^2(t)\rangle = \int \mathbf{P}(f) df .
\end{equation}
%
However, we only show the PSD up to the Nyquist frequency, so the
conversion from variance measured in the frequency domain to the time
domain requires and additional multiplcation by 2; hence the units are
pW$^2$\,Hz$^{-1}$ rather than pW$^2$\,s.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{pspec.pdf}
\caption{Bolometer power spectral densities. Four representative
  bolometers have been selected from each sub-array, and the
  flat-fielded (but otherwise raw) PSDs are shown as coloured dotted
  lines (the blue signals are for the same time series as those shown
  in Fig.~\ref{fig:bolos_mix}). Horizontal dotted lines at $5 \times
  10^{-8}$ and $5 \times 10^{-9}$\,pW\,Hz$^{-1}$ at 450 and
  850\,\micron\ respectively, give the approximate white-noise
  levels. There are broad line features in the PSDs at both
  wavelengths above $\sim$35\,Hz. From $\sim$50\,Hz up to the Nyquist
  frequency, the gradual roll-off of the anti-aliasing filter is also
  evident.  At lower frequencies, the bolometer signals exhibit clear
  $1/f$ knees at approximately 2 and 4\,Hz at 450 and
  850\,\micron. The solid black lines are the PSDs of the common-mode
  signals at each wavelength, and the solid coloured lines show the
  PSDs of the bolometers once the common-mode is removed. These
  residual signals have significantly lower $1/f$ knees, approximately
  0.4 and 2\,Hz at 450 and 850\,\micron. Finally, the dashed black
  lines show the spectral shape produced by a point source, assuming
  that the telescope is slewing at 200\,arcsec\,sec$^{-1}$ (typical
  for S2SRO); for reference, the top horizontal axis shows the
  conversion from frequency to angular scale. We can see that the line
  features may add significant noise on the scale of the PSF. We also
  see that, for this scan velocity, the low-frequency noise begins to
  dominate on scales of approximately 8.3 and 1.7\,arcmin; these scales
  are comparable to the array footprint.}
\label{fig:pspec}
\end{figure}

The dotted coloured lines in Fig~\ref{fig:pspec} show the PSDs for
raw, though flatfielded data. At each wavelength, there is a clear
$1/f$ knee at a few Hz, followed by a predominantly white spectrum
punctuated by line features above $\gsim 30$\,Hz, and finally roll-off
caused by the anti-aliasing filter above $\gsim 70$\,Hz. As indicated
in the previous section, the correlation between the low-frequency
components of the different bolometer signals is large. The solid
black lines in Fig.~\ref{fig:pspec} indicate the PSDs of the
common-modes $\mathbf{c}(t)$ at each wavelength, which reproduce most
of the low-frequency structures, as well as the high-frequency line
features. The $\mathbf{c}(t)$ otherwise drop substantially below the
individual bolometer PSDs at high-frequency as expected if the
bolometers are dominated by un-correlated white noise. Finally, we
note that the amplitudes and slopes of the $\mathbf{c}(t)$ are similar
at both 450 and 850\,\micron. However, since the 450\,\micron\ white
noise is larger than at 850\,\micron\ (shown approximately by the
horizontal dotted lines), the $1/f$ knee occurs at a \emph{lower}
frequency at 450\,\micron.

Next, the common-mode signals are fit to each bolometer time series
and removed as in Fig.~\ref{fig:bolos_mix}, and the resulting PSDs are
shown as solid coloured lines. In this example, the residual signals
have $1/f$ knees significantly lower than in the raw PSDs.

For reference, the top horizontal axis has been converted to angular
scale assuming a scan velocity of 200\,arcsec\,sec$^{-1}$ which was
typical of the S2SRO observations. The power spectra of the PSFs in
each band (arbitrary normalized to peak values of $10^{-6}$ and
$10^{-7}$\,pW$^2$\,Hz$^{-1}$ at 450 and 850\,\micron\ respectively)
are also showed as dashed lines for this assumed scan velocity showing
that the smallest features resolvable by the telescope may be slightly
affected by the excess noise in the line features. At the lower
frequency end, the $1/f$ noise dominates at scales $\gsim 2$\,arcmin
in the raw data and $\gsim 10$\,arcmin in the common-mode cleaned data
at 450\,\micron, and at scales $\gsim 1$\,arscmin in the raw data and
$\gsim 2$\,arcmin in the common-mode cleaned data at 850\,\micron.

\subsection{Magnetic field pickup}
\label{sec:magpickup}

An additional noise source that appears to be significant in only a
small subset of the S2SRO data is magnetic field pickup. Since the
bolometer signals are ultimately detected through the amplification of
magnetic fields, any additional changing fields within the cryostat
will also be detected.

An example of an observation where pickup appears to be significant is
shown in Fig.~\ref{fig:magpickup}. The time-series for two
450\,\micron\ bolometers in the same column (not flatfielded) show
that there is a strong signal with a similar shape, but opposite
signs. This behaviour is seen across the entire array. The dark squid
signal for the same columns exhibits a similar shape and
amplitude. Since the dark squid has no thermal absorber or TES
attached to it, this observed signal is not likely to be optical or
thermal in nature (although there can be some crosstalk with the
bolometers...). It is also for this reason that the higher-frequency
noise that appears in the bolometer data (thermal) does not appear in
the dark squid, leading to a significantly higher \snr. The sign of
such a signal is expected to be random in the bolometer time-series
(ask detector person for an explanation).

\begin{figure}
\centering
\includegraphics[width=\linewidth]{magpickup.pdf}
\caption{Evidence for significant magnetic field pickup for
  observation 20100228\_00016.  The top panel shows two un-flatfielded
  (but mean-subtracted) bolometer time-series from the same column,
  with a 200 sample boxcar smooth (approximately 1\,s), illustrating
  that they are dominated by a similar signal with opposite signs. The
  second panel shows the dark squid signal for the column, also
  mean-subtracted and with the same boxcar smooth. The bottom panel
  shows the azimuthal and elevation offsets from the map centre. For
  reference, the mean azimuth was 171.9\,deg, and the mean elevation
  68.0\,deg. Only the azimuthal signal appears to be correlated with
  the dark squids and bolometer signals, which suggests a magnetic
  field stationary with respect to the telescope dome as the source,
  since its direction with respect to the cryostat only changes with
  azimuthal motion.}
\label{fig:magpickup}
\end{figure}

The telescope pointing offsets for this approximately 0.5\,deg
diameter scan are also shown in Fig.~\ref{fig:magpickup}. Since the
phase of the azimuth offsets from the map centre in this scan pattern
slowly drifts with respect to the elevation offsets, it is clear that
the bolometer and dark squid signals are detecting a noise source that
is correlated only with the azimuthal motion and \emph{not} the
elevation. This behaviour would be expected if if there were a strong
magnetic field fixed with respect to the telescope dome (i.e., the
earth's magnetic field). Since SCUBA-2 is mounted on a Nasmyth
platform, only azimuthal motion will change the effective direction of
such a field with respect to the cryostat. It is also worth noting
that the absolute azimuth of the telescope was close to 180\,deg
(i.e., nearly due-south). We suspect that this observing
configuration, combined with the large amplitude of the scan, may have
conspired to produce these large signals.

%-------------------------------------------------
\subsection{Differences between the S2SRO and 2011 re-commissioned instrument}
%-------------------------------------------------

The S2SRO period took place during February and March 2010. For the
purpose of this paper, we will consider data taken as early as 2009
December 3, at which point the instrument was being commissioned in a
similar configuration to that of the S2SRO period. At this time, each
of the 450 and 850\,\micron\ focal planes were populated with single
subarrays, s4a and s8d respectively. There were two main mechanical
problems to contend with during this period. First, the fridge was not
able to cool the focal plane to the desired base temperature of
... which affected stability, particularly at 850\,\micron. Second,
there were problems with the subarray fabrication process which led to
large numbers of dead pixels, as well as large-scale variations in
$T_\mathrm{C}$ and the thermal conductivity, $G$. These latter issues
made it difficult to simulataneously place all of the TESs into the
optimal region of the superconducting transition, leading to
significantly poorer sensitivities.

While there were moderate variations from night-to-night during the
observing period, there were typically 600 to 900 working detectors in
each of the s4a and s8d subarrays. Fig.~\ref{fig:sensitivities} shows
representative responsivity maps for the two subarrays from the middle
of S2SRO based on the response to ramping the bolometer heaters.

%-------------------------------------------------
\subsection{Principal component analysis}
\label{sec:pca}
%-------------------------------------------------

A method that has been employed by teams analyzing BOLOCAM and AzTEC
data to identify and remove correlated noise is Principal Component
Analysis \citep[PCA,][]{laurent2005,scott2008}. We initially used PCA
to gain further insight into the nature of the \scuba\ noise
properties, but it is now also included it in SMURF as method for
cleaning the data.

The basic method is as follows: (i) a covariance matrix is built up
for all pairs $(i,j)$ of the $N$ bolometer time series,
$\langle\mathbf{b}_i(t),\mathbf{b}_j\rangle$; and (ii) a singular
value decomposition is used to identify a new set of statistically
uncorrelated eigenvectors, $\mathbf{\xi}_i(t)$ (i.e. whose covariance
matrix is diagonal), such that each of the bolometer time series is a
linear combination of the eigenvectors, or \emph{components}
i.e. $\mathbf{b}_i(t) = \bar{\mathbf{\xi}} \mathbf{\lambda}_i^T$,
where each row of the matrix $\bar{\mathbf{\xi}}$ is an eigenvector,
and $\mathbf{\lambda}_i^T$ is a column vector containing the
corresponding eigenvalues. The $\mathbf{\xi}_i$ are normalized by
their \rms, so that the amplitude of each component is stored in the
eigenvalues. In the earlier analyses mentioned, the low-frequency
noise is assumed to be encapsulated in those components with the
largest eigenvalues. Removing the projection of the time series along
those components has been used successfully to reduce $1/f$ noise
while retaining most of the signal in point-like sources. There is an
implicit assumption in this analysis that the noise properties are
stationary.

One unique feature of the SCUBA-2 data is that one can perform a PCA
analysis of both the 450 and 850\,\micron\ data simultaneously. The
first 6 most significant modes of our analysis are shown in
Fig.~\ref{fig:pca}, with the eigenvectors in the top panels, and maps
of the eigenvalues in each subarray in the bottom panels.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{pca.pdf}
\caption{The first six modes from a principal component analysis,
  ordered by decreasing significance, of the combined 450 and
  850\,\micron\ time-series bolometer data. For each component, the
  top plot shows the time series of the eigenvector, normalized by its
  \rms. The bottom coloured panels show the eigenvalues for the
  bolometers at each wavelength (the amplitude of the eigenvector in
  the time series). For reference, both the mean, $\bar{\lambda}$, and
  \rms, $\lambda_\mathrm{rms}$, eigenvalues for the bolometers in each
  subarray are also shown. The first two components have comparable
  amplitudes, and dominate by at least a factor of 10 any other
  components of the time series. We suspect that they are caused by
  atmospheric variations (expected to be brighter at 450\,\micron),
  and fridge oscillations (expected to be comparable at each
  wavelength). However, the two eigenvectors do not necessarily map
  directly to these physical sources under this interpretation; rather
  they are two \emph{different} linear combinations of the underlying
  physical signals that give rise to statistically uncorrelated
  components. The third component accurately reproduces an apparent
  data-acquisition glitch that appears only at 450\,\micron\ (see
  Section ???). The fourth component is a ...\,Hz line feature only
  observed in column 15 of the 450\,\micron\ subarray. The fifth
  component shows a series of steps and glitches that appear
  predominantly along the bottom-left edge of the 850\,\micron\
  subarray where the bolometers are only tenuously locked. Finally,
  the sixth component is degenerating into a less significant, and
  more randomly scattered signal across the focal plane. Subsequent
  components follow this trend, and would generally be kept in a
  PCA-cleaning procedure.}
\label{fig:pca}
\end{figure*}

The majority of correlated signal at both wavelengths is clearly
broken down into the first two plotted components. The first dominates
at 450\,micron, whereas the second dominates at 850\,micron. For
reference, the eigenvalues of these two modes are in the range
$\sim1$--4\,pW, whereas the remaining modes have eigenvalues
$\lsim0.3$\,pW. As mentioned earlier, one simple model for the
correlated signal is a simple combination of oscillations in base
temperature (which would be similar in each band), with a varying
atmospheric signal (which is expected to be stronger at
450\,\micron). Indeed, the fainter component 2 most closely resembles
the mixing chamber temperature variations (Fig.~\ref{fig:bolos_mix}),
whereas component 1 is stronger at 450\,\micron\ and does not resemble
the fridge signal as closely.

This physical interpretation of the eigenvectors should, however, be
taken with a grain of salt, since the PCA analysis is really a
statistical black box. Suppose most of the low frequency noise really
could be decomposed into two components, the fridge signal
$\mathbf{F}(t)$, and an atmospheric signal $\mathbf{A}(t)$. The two
eigenvectors obtained from the PCA analysis are simply linear
combinations of $\mathbf{F}(t)$ and $\mathbf{A}(t)$ that are
statistically uncorrelated. Depending on the particular realizations
of these two noise sources, this decomposition may, or may not give
rise to eigenvectors that resemble the two underlying physical
sources. Regardless, similar analyses of a few data sets during S2SRO
give comparable results; most of the signal at both 450\,\micron\ and
850\,\micron\ appears to be produced by two dominant underlying
mechanisms.

The subsequent components identified by the PCA analysis in
Fig.~\ref{fig:pca} vary wildly. Component 3, though not obvious from
its eigenvector plot, is almost a pure ramping waveform that only
appears in the s4a data, and produces the broad line featuress
centered over 45\,Hz and 70\,Hz in the 450\,\micron\ PSDs in
Fig.~\ref{fig:pspec}. In other data sets, this third component has
also often instead exhibited a regularly-spaced series of spikes with
heights quantized into several amplitude families (again, only in the
s4a array). In both cases, the shape and phase of this signal is the
same in each bolometer, although the amplitude (and sign) varies, as
evidenced by the large random scatter in the s4a eigenvalue maps. It
is thought that these features are some sort of data acquisition
glitch as the spikes (when they appear) exhibit the expected point
response function of the anti-aliasing filter. Similarly, component 4
also appears to be a data acquisition glitch in the s4a array,
although along a single column, and with a peak at a lower frequency
of 40\,Hz, and a second feature right at the Nyquist frequency.

The last two components in Fig.~\ref{fig:pca} have an entirely
different character. The amplitudes of both components are greatest in
low-responsivity regions of the two subarrays (lower-right at
450\,\micron\ and bottom-left at 850\,\micron). Bolometers in these
regions of the subarray are probably only tenuously locked, and are
more sensitive to changes in the loading, possibly resulting in the
step and spikes that are seen in component 5. Component 6 is clearly
related to component 5, though with the absence of the steps and
spikes, so that together they seem to represent a continuum of shapes
that are linear combinations of the spikes and steps with the slower
drift evident only in component 6.

%------------------------------------------------------------------------------
\section{The SMURF Algorithm}
\label{sec:algorithm}
%------------------------------------------------------------------------------

Given \scuba's data rate, SMURF was designed with two primary goals in
mind: (i) reduced maps should be scientifically useful and of ``near
publication quality'' with minimal user interaction; and (ii) the
execution time should not appreciably exceed the amount of time
required to conduct observations on a high-end multi-tcore desktop
computer. By attaining these goals it would be possible to conduct
real-time reductions at the telescope to offer feedback to observers,
and it would also be possible to maintain a central repository of
(nearly) publishable data products.

As described in the introduction, rather than pursue a slow but
potentially more accurate \emph{maximum likelihood} method that would
require the inversion of a complex linear problem, we instead use an
iterative approach that estimates and removes most of the correlated
(and in particular low-frequency) noise sources in parallel with a
simplified map estimation. We do note, however, that recent advances
in direct inversion methods such as \citet{patanchon2008} may in fact
be tenable for SCUBA-2 data. A future goal of the software team is to
investigate meshing SMURF (perhaps as a cleaning step and initial
condition) for a SANEPIC-like algorithm.

The remainder of this section gives an overview of the model that is
assumed by SMURF for the \scuba\ data, and the typical steps used to
estimate a map. Subsequent sections discuss problems with this model
and additional processing steps and constraints that can be used to
improve the solution in different cases.

\begin{itemize}

\item Describe the bolometer signal model, and with the help of a flow-chart
show the basic procedure that SMURF follows.

\item Discuss SMURF configurability

\item Some sort of proof/demonstration showing why the algorithm works

\item Discuss performance (execution time, memory, disk usage etc.)

\item Where do you get SMURF/starlink, usage of NDF

\item Talk about pipeline?

\end{itemize}

We express the signal observed by the $i$th bolometer as a function of time,
\begin{equation}
\mathbf{b}_i(t) = g_i[\mathbf{e}_i(t) \mathbf{a}_i(t) + \mathbf{n}_i(t)]
\end{equation}
where $\mathbf{a}(t)$ is the time-varying signal produced by scanning
the telescope across astronomical sources, $\mathbf{e}(t)$ is the
time-varying extinction, which is a function of the telescope
elevation and atmospheric conditions, and $\mathbf{n}_i(t)$ represents
sources of noise. The two terms in square brackets, as written, have
units of power delivered to the detectors (pW). The scale factor $g_i$
converts this effective power to the digitized units recorded on disk
(DAC) -- the \emph{flatfield} multiplied by a digitization constant --
and in this formulation it is assumed to be constant in time.

We then express the noise, $\mathbf{n}_i(t)$, as the sum of several
components,
%
\begin{equation}
  \mathbf{n}_i(t) = \mathbf{n}^\mathrm{w}_i(t) + \mathbf{n}^\mathrm{f}_i(t) +
  \mathbf{n}^\mathrm{c}(t),
\end{equation}
%
where $\mathbf{n}^\mathrm{w}_i(t)$ is uncorrelated white (thermal)
noise, $\mathbf{n}^\mathrm{f}_i(t)$ is low-frequency noise that is
\emph{not} correlated from bolometer-to-bolometer, and
$\mathbf{n}^\mathrm{c}(t)$ is a (predominantly low-frequency)
correlated or \emph{common-mode} component. The primary goal of
map-making is to model and remove $\mathbf{n}^\mathrm{f}_i$ and
$\mathbf{n}^\mathrm{c}$ from the time-series, while re-gridding the
remaining data we can then hope to approach the theoretical noise
limit in the map. This procedure is summarized in Fig.~\ref{fig:dimm}.

\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{dimm.pdf}
\caption{Typical map-making algorithm. Raw data (stored in multiple
  files) are read and concatenated into continuous time series. A
  pre-processing stage repairs DC steps, applies a flatfield
  correction, and identifies and removes the noisiest bolometers. The
  iterative portion then begins: estimating and removing the
  common-mode signal; applying the extinction correction; high-pass
  filtering to remove residual independent low-frequency noise;
  estimating the map by re-gridding the data; and finally measuring
  the noise in the residual time series. If the \rms\ has converged,
  the final map is written to disk. Otherwise any multiplicative
  factors that may have been applied to the data are inverted (e.g.,
  extinction correction and relative gains measured during the
  common-mode calculation), and each additive component is added back
  into the time series and re-estimated in turn.}
\label{fig:dimm}
\end{figure}

First, the data files are read from disk, concatenated into contiguous
arrays in memory, and run through a pre-processing stage that performs
several operations including spike and sudden level step removal to
enforce continuity in the time-series. Then, the iterative process
within the dashed box begins. It is generally assumed that the
dominant noise component is the common-mode, which is presumably
dominated by large variations in the fridge temperature and sky noise
(as argued in Section~\ref{sec:pca}). This signal is estimated as the
average signal observed by all of the bolometers at each time step,
and then removed from all of the bolometers. Once this large signal
has been removed, we apply the extinction correction, which is
determined completely from external measurements
\citep[see][]{dempsey2011}. Next, residual noise that differs from
bolometer-to-bolometer (primarily low-frequency) is removed, usually
using a high-pass filter. The resulting ``clean'' time-series are then
re-gridded to produce a map estimate using nearest-neighbour
sampling. Since each map pixel is calculated as the average of many
bolometer samples, it is fundamentally much less noisy than the raw
bolometer time-series. We can therefore project the map back into the
time domain (i.e. scanning the array across the current estimate of
the map), and create approximately noiseless estimates of what each
bolometer would have seen in the absence of noise sources, and remove
that from the time-series. This final step in the iteration gives a
residual signal (with low-frequency and astronomical noise components
removed) that may be used to measured the white noise level in each
detector, and also to estimate an approximate $\chi^2$ for the model
to track convergence. As we will discuss in the next sections, each
signal component estimate is biased slightly from the presence of
signal from other components, so the entire process is
iterated. However, before the next iteration begins, we first undo the
multiplicative factors introduced by the extinction correction, and
also relative gains that may optionally be estimated in the
common-mode estimation phase.

In practice, the map-maker is highly configurable, both in terms of
the order in which model components are fit (and whether they are fit
at all), and the details of \emph{how} they are fit. A summary of the
model components is given in the next section.

%-------------------------------------------------
\subsection{Model components}
\label{sec:components}
%-------------------------------------------------

In addition to specifying when each model component is calculated
(normally an attempt is made to order them by decreasing amplitude in
the time-series), there are a number of parameters that control their
operation. Table~\ref{tab:components} list the model components and
the typical order in which they are calculated. The following sections
describe their implementation in detail.

\begin{table}
  \caption{Summary of the model components that can be fit to
    \scuba\ time-series data with SMURF. Only the first group of
    models are typically fit to the data (\texttt{COM}--\texttt{NOI})
    in the indicated order. The remaining models
    (\texttt{DKS}--\texttt{PLN}) have only been included for
    completeness}
  \vspace{0.2cm}
  \centering
  \begin{tabular}{c|l}
    \hline
    Model & Description \\
    \hline
    \texttt{COM} & remove common-mode signal \\
    \texttt{GAI} & common-mode scaled to each bolometer \\
    \texttt{EXT} & extinction correction \\
    \texttt{FLT} & Fourier transform filter \\
    \texttt{AST} & map estimate of astronomical signal \\
    \texttt{NOI} & noise estimation \\
    \hline
    \texttt{DKS} & dark squid cleaning along columns \\
    \texttt{SMO} & time-domain smoothing filter \\
    \texttt{PLN} & 2-dimensional time-varying plane removal \\
    \hline
    \end{tabular}
  \label{tab:components}
\end{table}

\subsubsection{\texttt{COM,GAI}: common-mode estimation}
\label{sec:comgai}

ï»¿Fig.~\ref{fig:com} shows the time series from a selection of typical
bolometers.\footnote{The data have been flat-fielded and each time series
has been adjusted to a mean value of zero.} The similarity between most
bolometers is evident, and forms the common-mode signal - assumed to be a
consequence of variations in the atmospheric emission and fridge
temperature. This common-mode usually dominates the astronomical signal
for all but the brightest sources, and swamps faint extended structure.
The purpose of the COM and GAI models is to remove this common-mode
signal.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{com.pdf}
\caption{A selection of typical bolometer time series after flatfielding
and removal of a constant baseline. It can be seen that most bolometers
exhibit a common time variation overlayed with other features.}
\label{fig:com}
\end{figure}

The COM model is the common-mode signal itself. It is a single time
series that is estimated by finding the mean of all bolometers at each
time step. Bolometer values that have been flagged as unusable for any
reason are excluded from the mean.

Since bolometers have varying sensitivities, the amplitude of the
common-mode variations will also vary from bolometer to bolometer. So
comparing each bolometer time stream with the common-mode signal allows
an estimate of the bolometer sensitivity to be obtained. In practice, a
least squares linear fit is performed between the bolometer time series
and the common-mode to determine a gain and offset for each bolometer.
The gain and offset for each bolometer is known as the GAI model. Each
bolometer value can then optionally be scaled and shifted using these
values so that all bolometers share a common (but as yet unknown)
calibration. This provides an alternative( or additional) flatfielding
strategy to that described in in section~\ref{sec:bolos}.

An option exists to cater for time-varying sensitivities. If used, the
least squares fits described above are performed on short blocks of
contiguous time slices, providing multiple gain and offset values for
each bolometer (one pair for each block of time slices). The gain and
offset at any required time slice can then be found by interpolation
between these values.

It can be seen from Fig.~\ref{fig:com} that some bolometers depart
radically from the common-mode, indicating some problem within the
bolometer. Such bolometers are identified by calculating the Pearson
coefficient of correlation between each bolometer time stream and the
common mode. Bolometers for which the correlation coefficient is below a
specified limit, or which have a negative gain, are flagged as bad in
order to omit them from the final map. If the option described above for
handling time-varying sensitivities is used, then a correlation
coefficient can be determined for each individual block of time slices.
This allows individual bad blocks to be rejected from a bolometer time
stream, rather than rejecting the whole bolometer. This is beneficial for
bolometers that follow the common-mode for most of the time but suffer
from transient problems at specific times.

The common-mode is calculated from all bolometers that have not
previously been flagged as bad. Thus aberrant bolometers with low
correlation coefficients will be included, polluting the common-mode. For
this reason, an iterative approach is taken to estimating the common-mode.
The initial common-mode estimate is formed from all remaining bolometers.
Aberrant bolometers with low correlation coefficients are then identified
by comparing each bolometer time stream with the common-mode. Such
aberrant bolometers are flagged as bad, and a new estimate of the
common-mode is produced omitting the newly flagged bolometers. This
process is repeated until no further bolometers are rejected.

Any astronomical sources that are smaller than the array size will
contribute signal to some bolometers but not other bolometers, thus
biassing the simple mean used to estimate the common mode. However, on
each iteration of the map-making algorithm illustrated in
Fig.~\ref{fig:dimm}, a large fraction of the remaining astronomical
signal is extracted from the bolometer time series and transferred to the
output map, resulting in subsequent estimates of the common-mode being
more accurate.

Any extended astronomical emission on a scale comparable to or larger
than the spatial extent of the area used to estimate the common-mode will
contribute a similar signal to all bolometers. Therefore such extended
emission is indistinguishable from the other sources of common-mode
signal (fridge and atmosphere variations) and will be removed by the COM
model. This places a limit on the spatial extent of astronomical
structure that can be recovered.

For this reason, the usual practice is to estimate a single COM model by
examining data from all four sub-arrays in each waveband, since this
allow spatial structure on the scale of the whole focal plane to be
recovered. However, sometimes there is evidence that the common-mode
differs from one array to another, and so an option exists to estimate a
separate COM model for each individual subarray, with a consequent
lowering in the scale of spatial structure that can be recovered.

\subsubsection{\texttt{EXT}: extinction correction}
\label{sec:ext}

The extinction correction is normally derived using the JCMT water
vapour radiometer (WVM), and is not considered to be a free parameter
in the solution. For details on how it is calculated see
\citet{dempsey2011}. Note that its numerical value is calculated only
once, and simply applied as a scale factor in the iterative solution.

\subsubsection{\texttt{FLT}: fourier transform filter}
\label{sec:flt}

This model takes the FFT of the bolometer data, and can apply both
high- and low-pass filters, as well as notch filteres, at hard
frequency edges specified by the user. Alternatively, the frequency
edges of the filters may be defined in terms of an \emph{angular
  scale}, but converted into a frequency through knowledge of the mean
telescope slew speed. The time-series may optionally have apodization
applied before the transform to avoid ringing (primarily caused by
wrap-around discontinuities at the ends of the time-series). Finally,
a whitening filter may also be applied in which a simple $1/f^\alpha +
\mathrm{constant}$ is fit to the power spectrum of each bolometer, and
then the bolometer FFT is multiplied by its inverse (though normalized
to preserve the white noise level).  The signal that is \emph{removed}
from the time-series by this process are stored in the \texttt{FLT}
container array.

\subsubsection{\texttt{AST}: map estimation}
\label{sec:ast}

Map estimation is accomplished using a simple nearest-neighbour
resampling of the data onto a pre-defined map grid. For the $i$th map
pixel $\mathbf{m}(x_i,y_i)$, the brightness is estimated as the
weighted average of the bolometer data samples $\mathbf{b}_j$ that
land within that pixel (from any bolometer or point in time),
%
\begin{equation}
  \mathbf{m}(x_i,y_i) = \frac{\sum_j \mathbf{w}_j \mathbf{b}_j }
                             { \sum_j \mathbf{w}_j } .
\end{equation}
%
For the initial iteration the weights $\mathbf{w}_j$ are set to 1, but
subsequently they are set to $1/\sigma_j^2$, the estimated inverse
variance expected from the bolometer white noise levels as discussed
in Section~\ref{sec:noi}. This weighting scheme is sensible provided
that the bolometer data have no correlated (i.e., low-frequency)
noise.

In addition, a variance map $\mathbf{v}(x_j,y_j)$ is estimated. The
default behaviour is to estimate this weighted error on the mean given
the scatter in the weighted samples. This is accomplished by dividing
the \emph{biased weighted sample variance} by the number of samples
that went into the average (akin to the formula for standard error on
the mean, but accounting for weights),
%
\begin{equation}
\mathbf{v}(x_j,y_j) = \frac{\sum_j \mathbf{w}_j
                            \sum_j \mathbf{w}_j \mathbf{b}_j -
                            \left( \sum_j \mathbf{w}_j \mathbf{b}_j \right)^2 }
                           { N \left( \sum_j \mathbf{w}_j \right)^2 },
\end{equation}
%
where $N_j$ is the total number of bolometer samples that land in the
pixel. We decided not to use the \emph{unbiased} estimator since in
practice it would require accumulating an additional array of values
at every map pixel, and only results in a small difference where there
are less than $\sim10$ samples per pixel (a situation that is almost
never encountered in a SCUBA-2 map except in the edge pixels).

Finally, once the map estimation is complete, the map is projected
into the time domain (the signal that would be produced in each
bolometer by the signal represented by the map) and removed.

In addition to map estimation, the \texttt{AST} model can also be used
to perform map-based despiking of the time-series
(Section~\ref{sec:mapdespike}), and to apply constraints to the map to
improve convergences (Section~???).

\subsubsection{\texttt{NOI}: noise estimation}
\label{sec:noi}

The primary purpose of the noise component, \texttt{NOI} is to measure
the noise in each bolometer residual once all of the other models have
been fit and removed. This noise may then be used to estimate weights
for the data in subsequent iterations.

As in Section~\ref{sec:psd} the FFT of each bolometer time series is
taken, and the PSD calculated. An average white noise level is then
measured from 2 to 10\,Hz, a relatively clean region that lies above
the $1/f$ knee, but below the high-frequency line features for most of
the bolometer data taken during S2SRO (Fig.~\ref{fig:pspec}). Taking
this constant level for $\mathbf{P}(f)$ we then calculate the expected
variance of the time-series (in approximately 200\,Hz samples) using
Eq.~\ref{eq:psd}. If the bolometer noise were produced purely by
uncorrelated thermal sources (i.e. no other long-scale drifts), with
no high-frequency line features, and without the attenuation at even
higher frequencies by the anti-aliasing filter, this is the
theoretical noise limit of the detectors. These measured variances are
stored, and then used in subsequent iterations to weight the data
points when calculating the map estimate (Section~\ref{sec:ast}).

Since noise estimation is usually calculated as the final step, it is
the cleanest data for which some cleaning operations may be
performed. For this reason \texttt{NOI} can optionally use the
time-domain DC step fixer (Section~\ref{sec:steps}) and spike
detection (Section~\ref{sec:timedespike}).

\subsubsection{\texttt{DKS}: dark squid cleaning along columns}
\label{sec:dks}

Columns share significant magnetic field pickup which is measured by
the dark squids. This model has been included although it makes little
improvement, or injects noise since the magnetic field pickup noise is
generally sub-dominant. Furthermore, not every column that contains
working bolometers also has a working dark squid. An alternative is to
use the \texttt{TMP} model described below.

\textit{There are also the popped bolometers that could help fill the
gaps.}

\subsubsection{\texttt{TMP}: remove a scaled template}

Since the \texttt{DKS} cannot generally be used due to the missing
dark squids, an alternative is to use the telescope pointing as a
proxy for the signal due to magnetic field pickup
(Section~\ref{sec:magpickup}. Generally speaking the azimuth (or its
sine) as a function of time is used as a template that is simply
scaled and removed from each bolometer signal. Some modest
improvements can be achieved with this model.

\subsubsection{\texttt{SMO}: time-domain smoothing filter}
\label{sec:smo}

TJ

We also have the SMO filter that's useful for very short scans
because edge effects don't matter so much. Two sorts - median which is
more robust, or mean which is more accurate.

\subsubsection{\texttt{PLN}: 2d time-varying sky noise plane removal}
\label{sec:pln}

TJ

We can remove a plane at each timeslice but it doesn't seem to improve
things. Perhaps refer to other papers that claim to have seen coherent
2d structures in the sky noise?

\subsection{Model degeneracies}

A fundamental problem faced by the SMURF algorithm, as described, is
that there are degeneracies between model parameters. Thinking of each
pixel in the output map, as well as each sample in the various noise
model components, such as the common-mode and signal removed by the
Fourier filtering, any degeneracy can cause the solution to
diverge. Of particular concern are situations in which the map
diverges.

%-------------------------------------------------
\subsection{Data cleaning}
%-------------------------------------------------

Since we can do most of this stuff either as a pre-processing step or
iteratively, I thinks this goes in a generic-sounding subsection. This
can go here, or perhaps earlier?

\subsubsection{Down-sampling}

EC

\subsubsection{Whitening filter}

EC

\subsubsection{Time-domain de-spiking}
\label{sec:timedespike}

Spikes of short duration and high amplitude are often seen in the time
series data. If not removed, they can cause ringing in the FLT model
(Section~\ref{sec:flt}). Two alternative approaches may be used to remove
these spikes. This section describes the detection and removal of spikes
within the time series of each bolometer, and
section~\ref{sec:mapdespike} describes the detection and removal of
spikes within the final map. In practice, map-based despiking
(Section~\ref{sec:mapdespike}) usually gives superior results, and so
time-domain despiking is usually switched off.

\emph{Do we need some discussion of the physical causes and
characteristics of spikes?}

Each one-dimensional bolometer time series is processed independently. At
each time slice, the median value of the current bolometer in a box
centred on the time slice, and containing a specified number of time
slices (typically 50), is found. If the residual between the
time slice value and the median value is greater than some specified
multiple (typically 10) of the local noise level, the time slice is
flagged as a spike.

If the local noise level were estimated within the same box used to
determine the median value, a spike in the box would cause the local
noise level to be over-estimated severely. For this reason, the local
noise level is taken as the standard deviation of the values within a
neighbouring box on the ``down-stream'' side of the median box (that is,
the side that has already been checked for spikes). That is, the high end
of the noise box is just below the low end of the median filter box. This
introduces a slight asymmetry in the noise, but this should not matter
unless the noise varies significantly on a time scale shorter than the
box size.

This simple algorithm is not very good at distinguishing between spikes
and bright point sources, and so the threshold for spike detection is
usually raised when making maps of bright point sources.

\subsubsection{Step correction}
\label{sec:steps}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{steps1.pdf}
\caption{Examples of steps in time series data. Steps occur with a wide
range of heights from the large steps shown on the left to the small
steps shown on the right. Large steps are often followed by a brief
``over-shoot'' as shown on the left. In both plots, the black curve is
the uncorrected time series, and the red curve is the corrected time
series. Samples close to a step are omitted in the corrected time series.
In the right hand plot, the blue curve is the uncorrected time series for
a nearby bolometer. The similarity between the red and blue curves shows
that the step correction is performing well.
}
\label{fig:steps1}
\end{figure*}

Sudden steps can occur in the time stream data from each bolometer.

\emph{why?}

The black curves in Fig.~\ref{fig:steps1} show examples of such steps in
the time streams for two bolometers. If not removed, these steps can
cause severe ringing in the FLT model (Section~\ref{sec:flt}) and visible
streaks in the final map, corresponding to the paths of individual
bolometers over the sky.

Steps occur with a wide range of heights and shapes. The ratio of step
height to noise can vary from less than 10 to several hundred. Some steps
occur over a single sample such as the step close to sample 5000 in the
right-hand plot of Fig.~\ref{fig:steps1}, but others happen more
gradually such as the step close to sample 5300. In addition, a step can
be preceded or followed by a short period of instability, as is visible
at the bottom of the step in the left hand plot of
Fig.~\ref{fig:steps1}. Further problems are caused by steps that occur
close together in time, such as the large downward step followed by a
smaller upward step close to sample 5000 in the right hand plot of
Fig.~\ref{fig:steps1}.

Detecting and correcting such a wide variety of steps reliably has proved
to be a challenge. The current step-correction algorithm is described in
detail in appendix~\ref{appendix:steps}. In outline, the following stages
are involved in detecting steps in a single bolometer time stream:

\begin{enumerate}
\item Median smooth the whole time stream.
\item Find the gradient of the median smoothed time series at each sample.
\item Smooth the gradient values to determine the local mean gradient and
subtract this local mean from the total gradient to get residual gradients.
\item Find residual gradient values that exceed 25 times the local RMS of
the residual gradients.
\item Group these high residual gradients into contiguous blocks of
samples.
\item Merge blocks that are separated by less than 100 samples
\end{enumerate}

The above process produces a list of candidate steps in each bolometer
time series. Each candidate step is then verified, measured and corrected
using the following procedure:

\begin{enumerate}

\item The above process can mis-interpret the edges of a bright source
as a step. So we ignore blocks that occur close to bright sources.

\item If the block passes the above test, a least squares linear fit is
performed to the median-filtered bolometer data just before the block, and
this fit is extrapolated to predict the data value expected at the centre
of the block on the basis of the preceding data.

\item A least squares linear fit is performed to the median-filtered
bolometer data just after the block, and this fit is extrapolated to
predict the data value expected at the centre of the block on the basis
of the following data.

\item The difference between these two expected data values is taken as
the step height.

\item The preceding three steps are repeated several times, each time
including a different selection of samples in the two least squares fits.
The mean and standard deviation of the corresponding set of step heights
is found.

\item If the mean step height is small compared to the standard deviation
of the step heights, or compared to the noise in the bolometer data, then
the step is ignored.

\item If the above checks are passed, all subsequent bolometer samples are
corrected by the mean step height.

\item Bolometer samples within the duration of the step, and a few
samples on either side, are flagged as unusable.

\end{enumerate}

Once all steps have been corrected within a bolometer time stream, a
constant value is added to all samples in the time stream to restore its
original mean value.

The results of step correction are shown by the red curves in
Fig.~\ref{fig:steps1}. For comparison, the blue curve shows the
uncorrected time stream from a nearby bolometer that does not suffer from
steps. The agreement between the red and blue curves confirms that
the step correction algorithm is working satisfactorily.


\subsubsection{Gap filling / apodization}
\label{sec:gaps}

DSB

Discuss gap filling, ``new'' window function method, also apodization
and start/end polynomial continuity stuff. Should reference CMB
literature -- in particular look at \citet{stompor2002}.

\subsubsection{Flatfields}
\label{sec:flatfields}

TJ

Discuss heater ramps, normalizing off the common-mode signal
amplitude, astronomical sources, and the internal flatfield source.

\textit{Surely flatfields are discussed in the main SCUBA-2 paper so
don't really have any useful purpose here since they aren't
technically part of the map-maker.}

\subsubsection{Additional bad bolometer rejection}

TJ

Measuring the noise power spectra and throwing away outliers. Data
flagged bad by the DA system.

\subsubsection{Map-based despiking}
\label{sec:mapdespike}

It is possible to use the map estimate (Section~\ref{sec:ast}) to
provide additional despiking of the time-series. Unlike the
time-domain despiker (Section~\ref{sec:timedespike}), this calculation
uses the scatter in the population of samples used to estimate the map
pixel values (from different times and bolometers) to reject
outliers. This method is more robust against false-positive detections
of bright/compact astronomical sources since transient features in the
time-series are unlikely to occur by chance whenever a bolometer
crosses a specific location on the sky, whereas real astronomical
sources are at fixed spatial locations.

The estimated variance, $\mathbf{v_p}(x_i,y_i)$, of the normalized
weighted samples that land in the $i$th map pixel is simply the biased
weighted sample variance (i.e., the variance map value multiplied by
the number of samples)
%
\begin{equation}
  \mathbf{v_p}(x_i,y_i) = N_i \mathbf{v}(x_i,y_i).
\end{equation}

In order to compare the weighted \emph{differences} between the
samples and the map values, [$\mathbf{b}_j - \mathbf{m}(x_i,y_i)$] to
$\mathbf{v_p}$, they must be scaled appropriately. We define a
normalized difference, $\mathbf{d}_j$, in such a way that the variance
of this new variable gives the weighted sample variance of the
underlying data points,
%
\begin{eqnarray}
  \frac{\sum_j \mathbf{d}_j^2}{N} &=&
  \frac{\sum_j \mathbf{w}_j [\mathbf{b}_j - \mathbf{m}(x_i,y_i)]^2}
       {\sum_k \mathbf{w}_k} \\
   \Rightarrow \mathbf{d}_j^2 &=& \frac{N \mathbf{w}_j [\mathbf{b}_j -
       \mathbf{m}(x_i,y_i)]^2}{ \sum_k \mathbf{w}_k} .
\end{eqnarray}
%
The map-based despiker flags those $\mathbf{d}_j$ that are further
than some threshold number of standard deviations,
$\sqrt{\mathbf{v_p}}$, away from zero so that they are not used in
subsequent iterations.


%------------------------------------------------------------------------------
\section{Examples}
\label{sec:examples}
%------------------------------------------------------------------------------

\subsection{Point source}
Reduce a calibrator. Describe the use of a zero mask around the
source. Compare with \citet{wiebe2009} who used the same method to
constrain a maximum-likelihood map with poor cross-linking.

\subsection{Deep blind survey}
Heavy initial filtering, and only iteratively solve for map and
common-mode with edge constraints.

\textit{And matched filter is discussed here or superceded by
the 2D FFTs?}

\subsection{Bright extended emission}
The hardest thing to do with SCUBA-2 at the moment. Try to generate a
source/background mask to assist the map solution based on thresholds
in \snr. Compare with BOLOCAM stuff.

\textit{Or now an externally supplied mask.}

%------------------------------------------------------------------------------
\section{Summary and future work}
\label{sec:summary}
%------------------------------------------------------------------------------

\scuba\ is being re-commissioned. Maybe we will have some initial data
on fridge performance? If both it, and sky noise are relatively
well-behaved, might be able to restrict filtering to lower frequencies
and get larger-scale structures.

Larger array footprint also helps.

Talk about potential of Pascale et al. method to get closer to
max-likelihood map solution (handle maps at constant scan angles with
weights in Fourier space separately).

%------------------------------------------------------------------------------
\section{Acknowledgements}
%------------------------------------------------------------------------------

\textit{Presumably the standard telescope acknowledgment here plus a thank you
to CFI for funding the DR work. Ed should probably thank both CANFAR
and JAC for additionally funding although his JAC funding is noted in
the affiliation.}

%------------------------------------------------------------------------------
\bibliographystyle{mn2e}
\bibliography{mn-jour,refs}
%------------------------------------------------------------------------------

%------------------------------------------------------------------------------
\appendix
\section[]{Step correction in detail}

An outline of the step correction algorithm is given in
Section~\ref{sec:steps}. This appendix contains a more detailed
description.

Detecting and correcting steps in a single bolometer time stream involves:

\begin{enumerate}

\item Locating each candidate step.

\item Determining the widths of the candidate steps.

\item Determine the height of the step. Steps can vary enormously in height.
For instance the step in the left plot of Fig.~\ref{fig:steps1} is
roughly 3 pW in height, whereas the step at sample 5100 in the right hand
plot is only 0.01 pW in height. The height of a step is taken as the
difference in the local median value just before and just after the step.

\item Correcting all subsequent data values by the step height.

\item Once all steps have been corrected, a global offset is added to the
corrected bolometer time stream to ensure that the mean value before and
after step correction is unchanged.

\end{enumerate}

\subsection{Detecting candidate steps}
The first stage in the algorithm is to search the bolometer time stream for
contiguous blocks of samples that have unusually large mean gradients
compared to the neighbouring samples. Each such block of samples is
considered to be a candidate step. The starting and ending time of each
such block is noted. Once all candidate steps have been found for a
bolometer, the algorithm proceeds to the next stage. Candidate steps are
found as follows:

\begin{enumerate}

\item Smooth the bolometer time stream using a median filter of width
50 samples. This reduces the effects of the noise in the data. Using a
median filter rather than a mean filter helps to preserve the sharp edges
at each step, and is robust against spikes, etc.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{steps2.pdf}
\caption{A section of the right hand bolometer from  Fig.~\ref{fig:steps1}
with the median smoothed data overlayed in red.}
\label{fig:steps2}
\end{figure}

The red curve in Fig.~\ref{fig:steps2} shows the median smoothed data for
the right hand bolometer from Fig.~\ref{fig:steps1}.

\item At each time sample, find the difference between the two adjacent
values in the median filtered bolometer data. This is proportional to the
gradient of the median filtered bolometer data. Fig.~\ref{fig:steps3}
shows these differences for the sample bolometer shown in previous
figures. The high gradients at the three step are clearly visible at
indices 5040, 5100 and 5280.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{steps3.pdf}
\caption{Differences between adjacent values of the median smoothed data
in  Fig.~\ref{fig:steps2}.}
\label{fig:steps3}
\end{figure}

\item Steps are located by looking for difference values that exceed 25
times the RMS value of the nearby neighbouring differences. However, the
local RMS of the differences includes the gradient of the underlying
noise-free time series. So if there is a strong upward or downward trend
in the time series data, the RMS of the difference values will be higher,
potentially resulting in some steps being missed. For instance, at sample
5000 the overall trend in the time series is downwards, resulting in a
negative bias to the difference values.\footnote{This bias is not visible in
Fig.~\ref{fig:steps3} because of the large vertical range, but can be seen in
Fig.~\ref{fig:steps4} which has a smaller vertical scale.}

Therefore to enhance the reliability of the step detection, such biases
are removed before locating the steps. To do this, the differences are
smoothed using a mean filter, and the smoothed differences are removed.
The remaining residual difference values represent the gradient due to
steps, spikes, noise, etc. It is important that the smoothed differences
are not polluted by the large values caused by spikes and steps. So
before smoothing, differences of more than three times the RMS difference
(taken over the whole time stream) are identified and flagged. The RMS is
then re-calculated omitting the flagged values and a further two such
rejection iterations are performed. This process flags high gradient values
caused by spikes, jumps, point sources, etc. The difference values close
to a step are often not typical of those in the wider neighbourhood, so
each block of contiguous flagged difference values is doubled in width.
These flagged values are shown as black in Fig.~\ref{fig:steps4}, with
the remaining unflagged values shown in red.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{steps4.pdf}
\caption{The red curve shows the data from Fig.~\ref{fig:steps3} plotted with smaller upper
and lower limits. The blue curve shows the results of smoothing the red
curve. Points omitted from the red curve are shown in black.}
\label{fig:steps4}
\end{figure}

\item The differences for the remaining samples are smoothed
using a mean filter of width 50 samples. The smoothing process fills in small
holes in the array (up to 40 samples), but will not fill in larger holes.
Such larger holes are filled in later by linear interpolation. The
resulting smoothed differences (shown in blue in Fig.~\ref{fig:steps4})
provide an estimate of the local gradient of the underlying noise-free
median-filtered data stream.

\item The smoothed differences are subtracted from the total differences
to get the residual differences caused by noise, spikes, steps, etc.
These are show in black in Fig.~\ref{fig:steps5}.

\item The local RMS of the residual differences at each time sample is
found by smoothing the squared residual differences using a median filter
of width 200 samples, and then taking the square root of the smoothed values.
These are show in green in Fig.~\ref{fig:steps5}.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{steps5.pdf}
\caption{The black curve shows the residuals between the red and blue curves in
Fig.~\ref{fig:steps4}. Missing data is filled in by a combination of
smoothing and linear interpolation. The green curve is the local RMS of the data
over a box width of 200 samples.}
\label{fig:steps5}
\end{figure}

\item Blocks of contiguous samples for which the residual difference exceeds
25 times the local RMS of the residual differences are found. Any two
such blocks that are separated by fewer than 100 samples are merged into
a single block. Blocks are ignored if they fail any of the following
tests:

\begin{enumerate}

\item The width of the block must be less than 200 samples.

\item The total of all the SNR values (the absolute residual difference
divided by the local RMS of the residual differences) in the block must be
at least 25.

\item The total of all SNR values in the block must exceed half the value of
the largest single SNR value in the block.

\end{enumerate}

The second and third tests are intended to prevent spikes being treated as
two very close steps, since such steps will have SNR values of opposite
signs and will thus cancel out.

The data in Fig.~\ref{fig:steps5} results in two blocks being defined -
one between samples 5037 and 5102 and another between samples 5286 and
5296. Each of these blocks define the start and end of a candidate step.
The first block includes two visible steps, but they have been merged
into a single candidate step due to their small separation (less than 100
samples).

\end{enumerate}


\subsection{Measuring and confirming candidate steps}





%------------------------------------------------------------------------------


\end{document}
